{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S3rnalRkddlv",
    "outputId": "efdb9641-228f-49b7-d008-5ea3a58d80d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Requirement already satisfied: torch==1.8.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/Information Retrieval 1/hw2/requirements.txt (line 1)) (1.8.0)\n",
      "Requirement already satisfied: numpy==1.18.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/Information Retrieval 1/hw2/requirements.txt (line 2)) (1.18.1)\n",
      "Requirement already satisfied: matplotlib==3.1.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/Information Retrieval 1/hw2/requirements.txt (line 3)) (3.1.2)\n",
      "Requirement already satisfied: pandas==1.1.5 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/Information Retrieval 1/hw2/requirements.txt (line 4)) (1.1.5)\n",
      "Requirement already satisfied: tqdm==4.42.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/Information Retrieval 1/hw2/requirements.txt (line 5)) (4.42.0)\n",
      "Requirement already satisfied: six==1.14.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/Information Retrieval 1/hw2/requirements.txt (line 6)) (1.14.0)\n",
      "Requirement already satisfied: requests==2.22.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/drive/MyDrive/Information Retrieval 1/hw2/requirements.txt (line 7)) (2.22.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->-r /content/drive/MyDrive/Information Retrieval 1/hw2/requirements.txt (line 1)) (3.10.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.2->-r /content/drive/MyDrive/Information Retrieval 1/hw2/requirements.txt (line 3)) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.2->-r /content/drive/MyDrive/Information Retrieval 1/hw2/requirements.txt (line 3)) (1.3.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.2->-r /content/drive/MyDrive/Information Retrieval 1/hw2/requirements.txt (line 3)) (3.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.1.2->-r /content/drive/MyDrive/Information Retrieval 1/hw2/requirements.txt (line 3)) (0.11.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5->-r /content/drive/MyDrive/Information Retrieval 1/hw2/requirements.txt (line 4)) (2018.9)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->-r /content/drive/MyDrive/Information Retrieval 1/hw2/requirements.txt (line 7)) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->-r /content/drive/MyDrive/Information Retrieval 1/hw2/requirements.txt (line 7)) (1.24.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->-r /content/drive/MyDrive/Information Retrieval 1/hw2/requirements.txt (line 7)) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->-r /content/drive/MyDrive/Information Retrieval 1/hw2/requirements.txt (line 7)) (2021.10.8)\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# !pip install -r '/content/drive/MyDrive/Information Retrieval 1/hw2/requirements.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFOTGDa2dLIN"
   },
   "source": [
    "# Homework 2: Learning to Rank <a class=\"anchor\" id=\"toptop\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itWsQ3QmdLIS"
   },
   "source": [
    "**Submission instructions**:\n",
    "- The cells with the `# YOUR CODE HERE` denote that these sections are graded and you need to add your implementation.\n",
    "- Please use Python 3.6.5 and `pip install -r requirements.txt` to avoid version issues.\n",
    "- The notebook you submit has to have the student ids, separated by underscores (E.g., `12341234_12341234_12341234_hw1.ipynb`).\n",
    "- This will be parsed by a regexp, **so please double check your filename**.\n",
    "- Only one member of each group has to submit the file (**please do not compress the .ipynb file when you will submit it**) to canvas.\n",
    "- **Make sure to check that your notebook runs before submission**. A quick way to do this is to restart the kernel and run all the cells.  \n",
    "- Do not change the number of arugments in the given functions.\n",
    "- **Please do not delete/add new cells**. Removing cells **will** lead to grade deduction. \n",
    "- Note, that you are not allowed to use Google Colab.\n",
    "\n",
    "**Learning Goals**:\n",
    "- Offline LTR\n",
    "  - Learn how to implement pointwise, pairwise and listwise algorithms for learning to rank \n",
    "\n",
    "---\n",
    "**Recommended Reading**:\n",
    "  - Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hullender. Learning to rank using gradient descent. InProceedings of the 22nd international conference on Machine learning, pages 89–96, 2005.\n",
    "  - Christopher J Burges, Robert Ragno, and Quoc V Le. Learning to rank with nonsmooth cost functions. In Advances inneural information processing systems, pages 193–200, 2007\n",
    "  - (Sections 1, 2 and 4) Christopher JC Burges. From ranknet to lambdarank to lambdamart: An overview. Learning, 11(23-581):81, 2010\n",
    "  \n",
    "\n",
    "Additional Resources: \n",
    "- This assignment requires knowledge of [PyTorch](https://pytorch.org/). If you are unfamiliar with PyTorch, you can go over [these series of tutorials](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
    "\n",
    "In the previous assignment, you experimented with retrieval with different ranking functions and in addition, different document representations. \n",
    "\n",
    "This assignment deals directly with learning to rank (LTR). In offline LTR, You will learn how to implement methods from the three approaches associated with learning to rank: pointwise, pairwise and listwise. \n",
    "\n",
    "\n",
    "**Note:**\n",
    "  - The dataset used in this assignment is +100Mb in size. You may need around 2Gb of RAM for running the whole notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EBU2j4fdLIU"
   },
   "source": [
    "# Table of Contents  <a class=\"anchor\" id=\"top\"></a>\n",
    "\n",
    "[Back to top](#toptop)\n",
    "\n",
    "\n",
    "Table of contents:\n",
    "\n",
    "\n",
    " - [Chapter 1: Offline LTR](#o_LTR) (270 points)\n",
    "     - [Section 1: Dataset and Utility](#dataU) (- points)\n",
    "     - [Section 2: Pointwtise LTR](#pointwiseLTR) (55 points)\n",
    "     - [Section 3: Pairwise LTR](#pairwiseLTR) (35 points)\n",
    "     - [Section 4: Pairwise Speed-up RankNet](#SpairwiseLTR) (65 points)\n",
    "     - [Section 5: Listwise LTR](#listwiseLTR) (60 points)\n",
    "     - [Section 6: Evaluation](#evaluation1) (55 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UAzpsxh1M5_P",
    "outputId": "7722a61e-e9a1-453c-d3af-d2f416935b83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dataset\n",
      "  Downloading dataset-1.5.2-py2.py3-none-any.whl (18 kB)\n",
      "Collecting alembic>=0.6.2\n",
      "  Downloading alembic-1.7.7-py3-none-any.whl (210 kB)\n",
      "\u001b[K     |████████████████████████████████| 210 kB 7.4 MB/s \n",
      "\u001b[?25hCollecting banal>=1.0.1\n",
      "  Downloading banal-1.0.6-py2.py3-none-any.whl (6.1 kB)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.2 in /usr/local/lib/python3.7/dist-packages (from dataset) (1.4.32)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from alembic>=0.6.2->dataset) (4.11.2)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.2.0-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 6.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic>=0.6.2->dataset) (5.4.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.3.2->dataset) (1.1.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->alembic>=0.6.2->dataset) (3.10.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->alembic>=0.6.2->dataset) (3.7.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic>=0.6.2->dataset) (2.0.1)\n",
      "Installing collected packages: Mako, banal, alembic, dataset\n",
      "Successfully installed Mako-1.2.0 alembic-1.7.7 banal-1.0.6 dataset-1.5.2\n"
     ]
    }
   ],
   "source": [
    "!pip install dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YP_h_HkfM_Kr",
    "outputId": "eaa2cd1e-27f0-4181-8617-657e174a42a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.0.3.tar.gz (6.0 kB)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from evaluate) (1.0.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from evaluate) (1.1.5)\n",
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (from evaluate) (0.90)\n",
      "Requirement already satisfied: lightgbm in /usr/local/lib/python3.7/dist-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from lightgbm->evaluate) (1.4.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lightgbm->evaluate) (1.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->evaluate) (2018.9)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->evaluate) (1.14.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->evaluate) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->evaluate) (3.1.0)\n",
      "Building wheels for collected packages: evaluate\n",
      "  Building wheel for evaluate (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for evaluate: filename=evaluate-0.0.3-py3-none-any.whl size=6860 sha256=e88627bb408f59047309e6c8f450a0e71813ded5fcd91b82c62e3a79508cac40\n",
      "  Stored in directory: /root/.cache/pip/wheels/0e/81/86/5dd2648f1ca66017709d332ea7251e5443fac671670ee487a2\n",
      "Successfully built evaluate\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-AGOhXkQdLIU"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import itertools\n",
    "from argparse import Namespace\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import dataset\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwNSRV9hdLIW"
   },
   "source": [
    "# Chapter 1: Offline LTR <a class=\"anchor\" id=\"o_LTR\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqCwSIPsdLIW"
   },
   "source": [
    "A typical setup of learning to rank involves a feature vector constructed using a query-document pair, and a set of relevance judgements. You are given a set of triples (`query`, `document`, `relevance grade`); where relevance grade is an *ordinal* variable  with  5  grades,  for example: {`perfect`,`excellent`,`good`,`fair`,`bad`),  typically  labeled  by human annotators.  \n",
    "\n",
    "In this assignment, you are already given the feature vector for a given document and query pair. To access these vectors, see the following code cells (note: the dataset will be automatically downloaded & the first time the next cell runs, it will take a while!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOa6jSFIdLIX"
   },
   "source": [
    "## Section 1: Data and Utility <a class=\"anchor\" id=\"dataU\"></a>\n",
    "\n",
    "[Back to TOC](#top)\n",
    "\n",
    "First let's get familiar with the dataset and some utility methods for our implementations.\n",
    "\n",
    "### Section 1.1 Dataset stats\n",
    "\n",
    "| Split Name | \\# queries | \\# docs | \\# features |\n",
    "| :- | :--: | :--: | :--: |\n",
    "| train | 2735 | 85227 | 501 |\n",
    "| validation | 403 | 12794 | 501 |\n",
    "| test | 949 | 29881 | 501 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "id": "KPmMcTTSdLIX",
    "outputId": "8f68a5d3-adae-4217-e086-84d44b1f7190"
   },
   "outputs": [],
   "source": [
    "dataset.download_dataset()\n",
    "data = dataset.get_dataset()\n",
    "# there is only 1 fold for this dataset \n",
    "data = data.get_data_folds()[0]\n",
    "# read in the data\n",
    "data.read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "id": "xeyc-hsidLIY",
    "outputId": "f04e7092-7b73-487d-db07-9ae0ad670736"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 501\n",
      "Split: train\n",
      "\tNumber of queries 2735\n",
      "\tNumber of docs 85227\n",
      "Split: validation\n",
      "\tNumber of queries 403\n",
      "\tNumber of docs 12794\n",
      "Split: test\n",
      "\tNumber of queries 949\n",
      "\tNumber of docs 29881\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of features: {data.num_features}\")\n",
    "# print some statistics\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    print(f\"Split: {split}\")\n",
    "    split = getattr(data, split)\n",
    "    print(f\"\\tNumber of queries {split.num_queries()}\")\n",
    "    print(f\"\\tNumber of docs {split.num_docs()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTNPv2BjdLIZ"
   },
   "source": [
    "### Section 1.2 Utility classes/methods\n",
    "\n",
    "The following cells contain code that will be useful for the assigment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nDq8FKFCdLIZ"
   },
   "outputs": [],
   "source": [
    "# these is a useful class to create torch DataLoaders, and can be used during training\n",
    "class LTRData(Dataset):\n",
    "    def __init__(self, data, split):\n",
    "        split = {\n",
    "            \"train\": data.train,\n",
    "            \"validation\": data.validation,\n",
    "            \"test\": data.test\n",
    "        }.get(split)\n",
    "        assert split is not None, \"Invalid split!\"\n",
    "        features, labels = split.feature_matrix, split.label_vector\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.labels = torch.FloatTensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.features.size(0)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.features[i], self.labels[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "zfdhh1m9dLIZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 501]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "## example \n",
    "train_dl = DataLoader(LTRData(data, \"train\"), batch_size=32, shuffle=True)\n",
    "# this is how you would use it to quickly iterate over the train/val/test sets \n",
    "# - (of course, without the break statement!)\n",
    "for (x, y) in train_dl:\n",
    "    print(x.size(), y.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arZbjNWodLIa"
   },
   "source": [
    "`evaluate_model` evaluates a model, on a given split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1VxLI9BxdLIa"
   },
   "outputs": [],
   "source": [
    "# this function evaluates a model, on a given split\n",
    "def evaluate_model(pred_fn, split, batch_size=256, print_results=False, q_level=False):\n",
    "    dl = DataLoader(LTRData(data, split), batch_size=batch_size)\n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "    for (x, y) in tqdm(dl, desc=f'Eval ({split})', leave=False):\n",
    "        all_labels.append(y.squeeze().numpy())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = pred_fn(x)\n",
    "            all_scores.append(output.squeeze().numpy())\n",
    "            \n",
    "    split = {\n",
    "            \"train\": data.train,\n",
    "            \"validation\": data.validation,\n",
    "            \"test\": data.test\n",
    "    }.get(split)   \n",
    "    results = evaluate.evaluate2(np.asarray(all_scores), np.asarray(all_labels), print_results=print_results, q_level=q_level)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HuTvDDQ9dLIa"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "dcg: 83.3923 (20.55143)\n",
      "dcg@03: 4.8776 (3.08759)\n",
      "dcg@05: 6.5999 (3.82986)\n",
      "dcg@10: 9.9760 (5.20426)\n",
      "dcg@20: 15.0317 (6.80245)\n",
      "ndcg: 0.6707 (0.05882)\n",
      "ndcg@03: 0.1786 (0.12338)\n",
      "ndcg@05: 0.1813 (0.11127)\n",
      "ndcg@10: 0.2000 (0.10661)\n",
      "ndcg@20: 0.2261 (0.09474)\n",
      "precision@01: 0.0600 (0.23749)\n",
      "precision@03: 0.0933 (0.17689)\n",
      "precision@05: 0.0960 (0.13994)\n",
      "precision@10: 0.0980 (0.13489)\n",
      "precision@20: 0.0900 (0.10100)\n",
      "recall@01: 0.0026 (0.01049)\n",
      "recall@03: 0.0115 (0.02249)\n",
      "recall@05: 0.0193 (0.02813)\n",
      "recall@10: 0.0363 (0.04378)\n",
      "recall@20: 0.0712 (0.07351)\n",
      "relevant rank: 136.2091 (76.70487)\n",
      "relevant rank per query: 3217.2600 (1485.84720)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lcu1812\\Anaconda3\\envs\\IR\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "## example \n",
    "# function that scores a given feature vector e.g a network\n",
    "net = nn.Linear(501, 1)\n",
    "# the evaluate method accepts a function. more specifically, a callable (such as pytorch modules) \n",
    "def notwork(x):\n",
    "    return net(x)\n",
    "# evaluate the function\n",
    "_ = evaluate_model(notwork, \"validation\", print_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PI2qPpPdLIb"
   },
   "source": [
    "The next cell is used to generate reproducible results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "V5UALUledLIb"
   },
   "outputs": [],
   "source": [
    "# use to get reproducible results\n",
    "def seed(random_seed):\n",
    "    import random\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtYFQRV9dLIb"
   },
   "source": [
    "## Section 2: Pointwise LTR (55 points) <a class=\"anchor\" id=\"pointwiseLTR\"></a>\n",
    "\n",
    "[Back to TOC](#top)\n",
    "\n",
    "Let $x \\in \\mathbb{R}^d$ be an input feature vector, containing features for a query-document pair. Let $f: \\mathbb{R}^d \\rightarrow \\mathbb{R} $ be a function that maps this feature vector to a number $f(x)$ - either a relevance score (regression) or label (classification). The data $\\{x \\}$ are treated as feature vectors and the relevance judgements are treated as the target which we want to predict. \n",
    "\n",
    "In this section, you will implement a simple Pointwise model using either a regression loss, and use the train set to train this model to predict the relevance score. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFSj-GJHdLIc"
   },
   "source": [
    "### Section 2.1: Neural Model (25 points)\n",
    "\n",
    "In the following cell, you will implement a simple pointwise LTR model: \n",
    "- Use a neural network to learn a model with different loss functions, using the relevance grades as the label. Use the following parameters: \n",
    "  - Layers: $501 (input) \\rightarrow 256 \\rightarrow 1$ where each layer is a linear layer (`nn.Linear`) with a ReLu activation function (`nn.ReLU`) in between the layers. Use the default weight initialization scheme. (Hint: use `nn.Sequential` for a one-line forward function!)\n",
    "  - This network will also be used by other methods i.e Pairwise \n",
    "  \n",
    "You should implement the following three methods:\n",
    "- `__init__` (4 points)\n",
    "- `forward` (1 point)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "liyUwDOsdLId"
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (5 points)\n",
    "class NeuralModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the Pointwise neural network.\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        super(NeuralModule, self).__init__()\n",
    "        self.input_size = 501\n",
    "        self.batch_size = 256\n",
    "\n",
    "        # self.model = nn.Sequential(\n",
    "        #   nn.Linear(self.input_size, self.batch_size),\n",
    "        #   nn.ReLU(),\n",
    "        #   nn.Linear(self.batch_size, 1),\n",
    "        #   nn.Softmax()\n",
    "        # )\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "          nn.Linear(self.input_size, self.batch_size),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(self.batch_size, 1)\n",
    "        )\n",
    "\n",
    "        # self.model = nn.Sequential(\n",
    "        #   nn.Linear(self.input_size, self.batch_size),\n",
    "        #   nn.ReLU(),\n",
    "        #   nn.Linear(self.batch_size, 1),\n",
    "        #   nn.ReLU()\n",
    "        # )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Takes in an input feature matrix of size (N, 501) and produces the output \n",
    "        Input: x: a [N, 501] tensor\n",
    "        Output: a [N, 1] tensor\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        return self.model(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "uWnPsd9adLId"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralModule(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=501, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# check the network configuration - layer dimensions and configurations\n",
    "point_nn_reg = NeuralModule()\n",
    "print(point_nn_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "1eVpOY5DdLId"
   },
   "outputs": [],
   "source": [
    "# test the forward function\n",
    "n = 10\n",
    "inp = torch.rand(n, data.num_features)\n",
    "out = point_nn_reg(inp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "AOpgcnyM4fAV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([[-0.0673],\n",
      "        [-0.1189],\n",
      "        [-0.0692],\n",
      "        [-0.1825],\n",
      "        [-0.1208],\n",
      "        [-0.1132],\n",
      "        [-0.0693],\n",
      "        [-0.1583],\n",
      "        [-0.2442],\n",
      "        [-0.1882]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(inp.grad)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5J68YYUWdLId"
   },
   "source": [
    "**Implementation (20 points):**\n",
    "Implement `train_batch` function to compute the gradients (`backward()` function) and update the weights (`step()` function), using the specified loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "TNcaS8uTdLIe"
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (20 points)\n",
    "\n",
    "def train_batch(net, x, y, loss_fn, optimizer):\n",
    "    \"\"\"\n",
    "    Takes as input a batch of size N, i.e. feature matrix of size (N, 501), label vector of size (N), the loss function and optimizer for computing the gradients, \n",
    "    and updates the weights of the model.\n",
    "\n",
    "    Input:  x: feature matrix, a [N, 501] tensor\n",
    "            y: label vector, a [N] tensor\n",
    "            loss_fn: an implementation of a loss function\n",
    "            optimizer: an optimizer for computing the gradients (we use Adam)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    optimizer.zero_grad()\n",
    "    scores = net(x)\n",
    "    loss = loss_fn(scores, y)  # add requires_grad=True?\n",
    "    net.zero_grad()  # not sure if this is necessary => Lan: i think maybe not\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "7eUM93Y0dLIe"
   },
   "outputs": [],
   "source": [
    "# Please do not change this. This cell is used for grading.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_yy3Es4dLIe"
   },
   "source": [
    "\\#### Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVi7RR57dLIe"
   },
   "source": [
    "### Section 2.2: Loss Functions (5 points)\n",
    "Pointwise LTR algorithms use pointwise loss functions.\n",
    "Usually, the popular loss functions for pointwise LTR is Regression loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omZ2AqCQdLIe"
   },
   "source": [
    "**Implementation (5 points):**\n",
    "Implement regression loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "-2QEQQJxdLIf"
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (5 points)\n",
    "def pointwise_loss(output, target):\n",
    "    \"\"\"\n",
    "    Regression loss - returns a single number. \n",
    "    Make sure to use the MSE loss\n",
    "    output: (float) tensor, shape - [N, 1] \n",
    "    target: (float) tensor, shape - [N]. \n",
    "    \"\"\"\n",
    "    assert target.dim() == 1\n",
    "    assert output.size(0) == target.size(0)\n",
    "    assert output.size(1) == 1\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    target = target.view(1, -1)\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(output, target)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "eULGaInRdLIi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your results:[3.200000047683716, 6.360000133514404, 4.159999847412109, 5.400000095367432, 3.240000009536743]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lcu1812\\Anaconda3\\envs\\IR\\lib\\site-packages\\torch\\nn\\modules\\loss.py:528: UserWarning: Using a target size (torch.Size([1, 5])) that is different to the input size (torch.Size([5, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "## Test pointwise_loss\n",
    "g = torch.manual_seed(42)\n",
    "output = [torch.randint(low=0, high=5, size=(5, 1), generator=g).float() for _ in range(5)]\n",
    "target = torch.randint(low=0, high=5, size=(5,), generator=g).float()\n",
    "\n",
    "l = [pointwise_loss(o, target).item() for o in output]\n",
    "print(f'your results:{l}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uXWJ3vOo0jml"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mp0OolqhdLIi"
   },
   "source": [
    "**Implementation (25 points):**\n",
    "Now implement a wrapper for training a pointwise LTR, that takes the model as input and trains the model.\n",
    "\n",
    "**Rubric:**\n",
    " - Network is trained for specified epochs, and iterates over the entire dataset and (train) data is shuffled : 5 points\n",
    " - Evaluation on the validation set: 5 points\n",
    " - Performance as expected: 15 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "fqKeqwrJdLIi"
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (25 points)\n",
    "def train_pointwise(net, params):\n",
    "    \"\"\"\n",
    "    This function should train a Pointwise network. \n",
    "    \n",
    "    The network is trained using the Adam optimizer\n",
    "        \n",
    "    \n",
    "    Note: Do not change the function definition! \n",
    "    \n",
    "    \n",
    "    Hints:\n",
    "    1. Use the LTRData class defined above\n",
    "    2. Do not forget to use net.train() and net.eval()\n",
    "    \n",
    "    Inputs:\n",
    "            net: the neural network to be trained\n",
    "\n",
    "            params: params is an object which contains config used in training \n",
    "                (eg. params.epochs - the number of epochs to train). \n",
    "                For a full list of these params, see the next cell. \n",
    "    \n",
    "    Returns: a dictionary containing: \"metrics_val\" (a list of dictionaries) and \n",
    "             \"metrics_train\" (a list of dictionaries). \n",
    "             \n",
    "             \"metrics_val\" should contain metrics (the metrics in params.metrics) computed\n",
    "             after each epoch on the validation set (metrics_train is similar). \n",
    "             You can use this to debug your models.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    val_metrics_epoch = []\n",
    "    train_metrics_epoch = []\n",
    "    optimizer = Adam(net.parameters(), lr=params.lr)\n",
    "    loss_fn = pointwise_loss\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    for i in range(params.epochs):\n",
    "      train_dl = DataLoader(LTRData(data, \"train\"), batch_size=params.batch_size, shuffle=True)\n",
    "      for (x, y) in train_dl:\n",
    "        net.input_size = x.size()[1]\n",
    "        net.batch_size = params.batch_size\n",
    "        break\n",
    "\n",
    "      for (x, y) in train_dl:\n",
    "          train_batch(net, x, y, loss_fn, optimizer)\n",
    "\n",
    "      train_metrics_epoch.append(evaluate_model(net, \"train\", print_results=True))\n",
    "\n",
    "      # validation\n",
    "      val_dl = DataLoader(LTRData(data, \"validation\"), batch_size=params.batch_size, shuffle=True)\n",
    "\n",
    "      val_metrics_epoch.append(evaluate_model(net, \"validation\", print_results=True))\n",
    "    \n",
    "    return {\n",
    "        \"metrics_val\": val_metrics_epoch,\n",
    "        \"metrics_train\": train_metrics_epoch\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "TEyg3wDQdLIi"
   },
   "outputs": [],
   "source": [
    "# Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-W1k8v8XdLIi"
   },
   "source": [
    "\\#### Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "jViJ7si6dLIi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lcu1812\\Anaconda3\\envs\\IR\\lib\\site-packages\\torch\\nn\\modules\\loss.py:528: UserWarning: Using a target size (torch.Size([1, 256])) that is different to the input size (torch.Size([256, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\lcu1812\\Anaconda3\\envs\\IR\\lib\\site-packages\\torch\\nn\\modules\\loss.py:528: UserWarning: Using a target size (torch.Size([1, 235])) that is different to the input size (torch.Size([235, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "dcg: 83.5755 (20.63332)\n",
      "dcg@03: 3.8167 (3.73266)\n",
      "dcg@05: 5.4595 (4.29418)\n",
      "dcg@10: 8.6765 (5.52771)\n",
      "dcg@20: 13.8015 (6.83561)\n",
      "ndcg: 0.6735 (0.05517)\n",
      "ndcg@03: 0.1372 (0.12716)\n",
      "ndcg@05: 0.1502 (0.11212)\n",
      "ndcg@10: 0.1739 (0.10224)\n",
      "ndcg@20: 0.2091 (0.09083)\n",
      "precision@01: 0.0781 (0.26829)\n",
      "precision@03: 0.0791 (0.15951)\n",
      "precision@05: 0.0871 (0.13327)\n",
      "precision@10: 0.0907 (0.11016)\n",
      "precision@20: 0.0959 (0.08625)\n",
      "recall@01: 0.0042 (0.01981)\n",
      "recall@03: 0.0104 (0.02566)\n",
      "recall@05: 0.0201 (0.03621)\n",
      "recall@10: 0.0392 (0.04743)\n",
      "recall@20: 0.0821 (0.06928)\n",
      "relevant rank: 119.8847 (71.61486)\n",
      "relevant rank per query: 2848.0721 (1463.24512)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "dcg: 82.7339 (20.18527)\n",
      "dcg@03: 3.4852 (3.16853)\n",
      "dcg@05: 4.5477 (3.47180)\n",
      "dcg@10: 7.7390 (4.14954)\n",
      "dcg@20: 13.1668 (5.74224)\n",
      "ndcg: 0.6656 (0.05877)\n",
      "ndcg@03: 0.1285 (0.12856)\n",
      "ndcg@05: 0.1266 (0.10859)\n",
      "ndcg@10: 0.1559 (0.09203)\n",
      "ndcg@20: 0.1996 (0.09229)\n",
      "precision@01: 0.0600 (0.23749)\n",
      "precision@03: 0.0800 (0.15720)\n",
      "precision@05: 0.0640 (0.12290)\n",
      "precision@10: 0.0740 (0.08902)\n",
      "precision@20: 0.0880 (0.08219)\n",
      "recall@01: 0.0024 (0.01102)\n",
      "recall@03: 0.0101 (0.02149)\n",
      "recall@05: 0.0142 (0.02665)\n",
      "recall@10: 0.0356 (0.04865)\n",
      "recall@20: 0.0758 (0.06842)\n",
      "relevant rank: 117.7180 (72.27686)\n",
      "relevant rank per query: 2780.5000 (1383.74702)\n"
     ]
    }
   ],
   "source": [
    "# Change this to test your code!\n",
    "pointwise_test_params = Namespace(epochs=1, \n",
    "                    lr=1e-3,\n",
    "                    batch_size=256,\n",
    "                   metrics={\"ndcg\"})\n",
    "# uncomment to test your code\n",
    "# train a regression model\n",
    "met_reg = train_pointwise(point_nn_reg, pointwise_test_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4KZSdR2dLIj"
   },
   "source": [
    "The next cell is used to generate results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "n_266yLsdLIj"
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def create_results(net, train_fn, prediction_fn, *train_params):\n",
    "    \n",
    "    print(\"Training Model\")\n",
    "    metrics = train_fn(net, *train_params)\n",
    "    net.eval()\n",
    "    test_metrics, test_qq = evaluate_model(prediction_fn, \"test\", print_results=True, q_level=True)\n",
    "    \n",
    "    \n",
    "    test_q = {}\n",
    "    for m in {\"ndcg\", \"precision@05\", \"recall@05\"}:\n",
    "        test_q[m] = test_qq[m]\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        \"metrics\": metrics,\n",
    "        \"test_metrics\": test_metrics,\n",
    "        \"test_query_level_metrics\": test_q,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDjyjxoudLIj"
   },
   "source": [
    "Now use the above functions to generate your results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fLGesdpzdLIj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "dcg: 87.6884 (20.76704)\n",
      "dcg@03: 5.9154 (4.68019)\n",
      "dcg@05: 8.1565 (5.20717)\n",
      "dcg@10: 12.2291 (6.24992)\n",
      "dcg@20: 18.7541 (7.66434)\n",
      "ndcg: 0.7081 (0.05587)\n",
      "ndcg@03: 0.2199 (0.17480)\n",
      "ndcg@05: 0.2290 (0.14949)\n",
      "ndcg@10: 0.2492 (0.12651)\n",
      "ndcg@20: 0.2870 (0.10587)\n",
      "precision@01: 0.1682 (0.37402)\n",
      "precision@03: 0.1572 (0.22145)\n",
      "precision@05: 0.1604 (0.17430)\n",
      "precision@10: 0.1471 (0.12956)\n",
      "precision@20: 0.1462 (0.10161)\n",
      "recall@01: 0.0082 (0.02373)\n",
      "recall@03: 0.0216 (0.03523)\n",
      "recall@05: 0.0373 (0.04551)\n",
      "recall@10: 0.0679 (0.06329)\n",
      "recall@20: 0.1301 (0.08611)\n",
      "relevant rank: 105.7271 (70.91133)\n",
      "relevant rank per query: 2511.7327 (1360.05899)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "dcg: 85.9730 (20.84005)\n",
      "dcg@03: 4.3483 (3.72987)\n",
      "dcg@05: 6.1761 (4.59519)\n",
      "dcg@10: 10.1527 (5.07312)\n",
      "dcg@20: 16.6332 (6.36849)\n",
      "ndcg: 0.6916 (0.05844)\n",
      "ndcg@03: 0.1577 (0.13644)\n",
      "ndcg@05: 0.1684 (0.12798)\n",
      "ndcg@10: 0.2017 (0.10473)\n",
      "ndcg@20: 0.2513 (0.09526)\n",
      "precision@01: 0.0800 (0.27129)\n",
      "precision@03: 0.0667 (0.16330)\n",
      "precision@05: 0.0840 (0.14472)\n",
      "precision@10: 0.1020 (0.10098)\n",
      "precision@20: 0.1170 (0.08866)\n",
      "recall@01: 0.0032 (0.01085)\n",
      "recall@03: 0.0088 (0.02125)\n",
      "recall@05: 0.0171 (0.03215)\n",
      "recall@10: 0.0423 (0.04047)\n",
      "recall@20: 0.1019 (0.06999)\n",
      "relevant rank: 105.3480 (70.62151)\n",
      "relevant rank per query: 2488.3200 (1314.38955)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "dcg: 87.4219 (20.48560)\n",
      "dcg@03: 5.7192 (4.41381)\n",
      "dcg@05: 7.8499 (5.11362)\n",
      "dcg@10: 12.0139 (6.16269)\n",
      "dcg@20: 18.5657 (7.34345)\n",
      "ndcg: 0.7064 (0.05676)\n",
      "ndcg@03: 0.2139 (0.17150)\n",
      "ndcg@05: 0.2224 (0.15231)\n",
      "ndcg@10: 0.2456 (0.12906)\n",
      "ndcg@20: 0.2855 (0.10703)\n",
      "precision@01: 0.1351 (0.34187)\n",
      "precision@03: 0.1542 (0.23162)\n",
      "precision@05: 0.1514 (0.18251)\n",
      "precision@10: 0.1465 (0.13544)\n",
      "precision@20: 0.1447 (0.10337)\n",
      "recall@01: 0.0068 (0.02263)\n",
      "recall@03: 0.0216 (0.03824)\n",
      "recall@05: 0.0344 (0.04555)\n",
      "recall@10: 0.0662 (0.06597)\n",
      "recall@20: 0.1276 (0.08798)\n",
      "relevant rank: 106.7682 (70.98944)\n",
      "relevant rank per query: 2536.4655 (1395.71578)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "dcg: 86.2562 (20.81515)\n",
      "dcg@03: 5.0361 (4.34146)\n",
      "dcg@05: 7.0730 (4.97449)\n",
      "dcg@10: 10.9543 (5.98617)\n",
      "dcg@20: 17.0124 (7.35729)\n",
      "ndcg: 0.6947 (0.05970)\n",
      "ndcg@03: 0.1838 (0.17245)\n",
      "ndcg@05: 0.1979 (0.15867)\n",
      "ndcg@10: 0.2218 (0.13268)\n",
      "ndcg@20: 0.2582 (0.11349)\n",
      "precision@01: 0.1600 (0.36661)\n",
      "precision@03: 0.1000 (0.17951)\n",
      "precision@05: 0.1080 (0.15600)\n",
      "precision@10: 0.1160 (0.11723)\n",
      "precision@20: 0.1210 (0.10397)\n",
      "recall@01: 0.0068 (0.01761)\n",
      "recall@03: 0.0113 (0.02098)\n",
      "recall@05: 0.0225 (0.03230)\n",
      "recall@10: 0.0510 (0.05085)\n",
      "recall@20: 0.1015 (0.07432)\n",
      "relevant rank: 106.2396 (71.00438)\n",
      "relevant rank per query: 2509.3800 (1285.74067)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "dcg: 80.5085 (19.79949)\n",
      "dcg@03: 2.8082 (2.86798)\n",
      "dcg@05: 4.0725 (3.57232)\n",
      "dcg@10: 6.5368 (4.35988)\n",
      "dcg@20: 10.5406 (5.48173)\n",
      "ndcg: 0.6490 (0.05431)\n",
      "ndcg@03: 0.1020 (0.10021)\n",
      "ndcg@05: 0.1128 (0.09647)\n",
      "ndcg@10: 0.1324 (0.08698)\n",
      "ndcg@20: 0.1618 (0.08067)\n",
      "precision@01: 0.0541 (0.22612)\n",
      "precision@03: 0.0561 (0.14218)\n",
      "precision@05: 0.0565 (0.11571)\n",
      "precision@10: 0.0619 (0.08605)\n",
      "precision@20: 0.0631 (0.07019)\n",
      "recall@01: 0.0022 (0.01003)\n",
      "recall@03: 0.0068 (0.01925)\n",
      "recall@05: 0.0131 (0.03243)\n",
      "recall@10: 0.0271 (0.04346)\n",
      "recall@20: 0.0539 (0.06123)\n",
      "relevant rank: 135.8634 (71.21381)\n",
      "relevant rank per query: 3227.6727 (1588.93839)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "dcg: 79.7365 (19.95414)\n",
      "dcg@03: 3.0288 (3.17210)\n",
      "dcg@05: 3.9859 (3.44657)\n",
      "dcg@10: 6.0371 (3.80348)\n",
      "dcg@20: 9.9539 (4.98129)\n",
      "ndcg: 0.6401 (0.05442)\n",
      "ndcg@03: 0.1107 (0.12474)\n",
      "ndcg@05: 0.1100 (0.10013)\n",
      "ndcg@10: 0.1186 (0.07163)\n",
      "ndcg@20: 0.1473 (0.06610)\n",
      "precision@01: 0.0600 (0.23749)\n",
      "precision@03: 0.0533 (0.16813)\n",
      "precision@05: 0.0440 (0.10800)\n",
      "precision@10: 0.0420 (0.07236)\n",
      "precision@20: 0.0450 (0.04924)\n",
      "recall@01: 0.0022 (0.00874)\n",
      "recall@03: 0.0059 (0.01834)\n",
      "recall@05: 0.0078 (0.01925)\n",
      "recall@10: 0.0167 (0.03084)\n",
      "recall@20: 0.0346 (0.03622)\n",
      "relevant rank: 133.9941 (70.36165)\n",
      "relevant rank per query: 3164.9400 (1512.17649)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "dcg: 84.7366 (20.71337)\n",
      "dcg@03: 4.5446 (4.26907)\n",
      "dcg@05: 6.3436 (4.97889)\n",
      "dcg@10: 10.0384 (6.04700)\n",
      "dcg@20: 15.6396 (7.51946)\n",
      "ndcg: 0.6833 (0.05794)\n",
      "ndcg@03: 0.1654 (0.15438)\n",
      "ndcg@05: 0.1763 (0.13803)\n",
      "ndcg@10: 0.2028 (0.11867)\n",
      "ndcg@20: 0.2385 (0.10467)\n",
      "precision@01: 0.1081 (0.31052)\n",
      "precision@03: 0.1211 (0.21693)\n",
      "precision@05: 0.1201 (0.16617)\n",
      "precision@10: 0.1216 (0.12283)\n",
      "precision@20: 0.1159 (0.09642)\n",
      "recall@01: 0.0056 (0.02302)\n",
      "recall@03: 0.0175 (0.03951)\n",
      "recall@05: 0.0273 (0.04448)\n",
      "recall@10: 0.0547 (0.05922)\n",
      "recall@20: 0.1027 (0.08009)\n",
      "relevant rank: 117.1470 (72.92602)\n",
      "relevant rank per query: 2783.0330 (1442.75487)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "dcg: 84.1099 (20.48391)\n",
      "dcg@03: 4.2316 (3.78893)\n",
      "dcg@05: 6.0337 (4.92721)\n",
      "dcg@10: 9.6146 (6.03892)\n",
      "dcg@20: 14.9999 (6.84857)\n",
      "ndcg: 0.6764 (0.05003)\n",
      "ndcg@03: 0.1495 (0.13199)\n",
      "ndcg@05: 0.1621 (0.12874)\n",
      "ndcg@10: 0.1876 (0.11153)\n",
      "ndcg@20: 0.2231 (0.08913)\n",
      "precision@01: 0.1000 (0.30000)\n",
      "precision@03: 0.0933 (0.17689)\n",
      "precision@05: 0.1040 (0.17545)\n",
      "precision@10: 0.1040 (0.11655)\n",
      "precision@20: 0.1090 (0.08043)\n",
      "recall@01: 0.0034 (0.01071)\n",
      "recall@03: 0.0108 (0.02139)\n",
      "recall@05: 0.0222 (0.04082)\n",
      "recall@10: 0.0481 (0.06455)\n",
      "recall@20: 0.1027 (0.08304)\n",
      "relevant rank: 114.6706 (72.60833)\n",
      "relevant rank per query: 2708.5200 (1388.04090)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "dcg: 87.1830 (20.84038)\n",
      "dcg@03: 5.8873 (4.90269)\n",
      "dcg@05: 8.0819 (5.51300)\n",
      "dcg@10: 12.2082 (6.64365)\n",
      "dcg@20: 18.4505 (7.89588)\n",
      "ndcg: 0.7038 (0.05881)\n",
      "ndcg@03: 0.2159 (0.17694)\n",
      "ndcg@05: 0.2258 (0.15427)\n",
      "ndcg@10: 0.2479 (0.13298)\n",
      "ndcg@20: 0.2821 (0.11061)\n",
      "precision@01: 0.1562 (0.36300)\n",
      "precision@03: 0.1642 (0.24455)\n",
      "precision@05: 0.1610 (0.18738)\n",
      "precision@10: 0.1498 (0.14028)\n",
      "precision@20: 0.1428 (0.10735)\n",
      "recall@01: 0.0080 (0.02502)\n",
      "recall@03: 0.0237 (0.04399)\n",
      "recall@05: 0.0378 (0.05149)\n",
      "recall@10: 0.0687 (0.06609)\n",
      "recall@20: 0.1249 (0.08715)\n",
      "relevant rank: 108.6184 (72.31338)\n",
      "relevant rank per query: 2580.4204 (1367.56703)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (validation):   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"metric\": \"mean\" (\"standard deviation\")\n",
      "dcg: 86.8319 (22.00724)\n",
      "dcg@03: 5.2795 (5.62539)\n",
      "dcg@05: 7.3309 (7.10031)\n",
      "dcg@10: 11.4262 (7.82950)\n",
      "dcg@20: 19.0270 (9.20521)\n",
      "ndcg: 0.6977 (0.05615)\n",
      "ndcg@03: 0.1835 (0.18324)\n",
      "ndcg@05: 0.1930 (0.17250)\n",
      "ndcg@10: 0.2216 (0.13465)\n",
      "ndcg@20: 0.2826 (0.11122)\n",
      "precision@01: 0.1600 (0.36661)\n",
      "precision@03: 0.1267 (0.23935)\n",
      "precision@05: 0.1280 (0.21821)\n",
      "precision@10: 0.1300 (0.14731)\n",
      "precision@20: 0.1570 (0.11576)\n",
      "recall@01: 0.0058 (0.01386)\n",
      "recall@03: 0.0140 (0.02782)\n",
      "recall@05: 0.0237 (0.03937)\n",
      "recall@10: 0.0565 (0.05863)\n",
      "recall@20: 0.1438 (0.09432)\n",
      "relevant rank: 105.4149 (73.10083)\n",
      "relevant rank per query: 2489.9000 (1256.29543)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d79ca4db216e47adb963bab87ef041db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval (train):   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed(42)\n",
    "params_regr = Namespace(epochs=11, \n",
    "                    lr=1e-3,\n",
    "                    batch_size=256,\n",
    "                    metrics={\"ndcg\", \"precision@05\", \"recall@05\"})\n",
    "\n",
    "pointwise_regression_model = NeuralModule()\n",
    "pw_regr = create_results(pointwise_regression_model, \n",
    "                          train_pointwise, \n",
    "                          pointwise_regression_model,\n",
    "                          params_regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z-NyrgZSdLIj"
   },
   "outputs": [],
   "source": [
    "assert \"test_metrics\" in pw_regr.keys()\n",
    "assert \"ndcg\" in pw_regr[\"test_metrics\"].keys()\n",
    "assert \"precision@05\" in pw_regr[\"test_metrics\"].keys()\n",
    "assert \"recall@05\" in pw_regr[\"test_metrics\"].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2IxtxbfdLIj"
   },
   "source": [
    "## Section 3: Pairwise LTR (35 points) <a class=\"anchor\" id=\"pairwiseLTR\"></a>\n",
    "\n",
    "[Back to TOC](#top)\n",
    "\n",
    "In this section,  you will learn and implement RankNet, a  pairwise learning to rank algorithm.\n",
    "\n",
    "For a given query, consider two documents $D_i$ and $D_j$ with two different ground truth relevance  labels,  with  feature  vectors $x_i$ and $x_j$ respectively.   The  RankNet  model,  just  like  the pointwise model, uses $f$ to predict scores i.e $s_i=f(x_i)$ and $s_j=f(x_j)$, but uses a different loss during  training. $D_i \\triangleright D_j$ denotes  the  event  that $D_i$ should  be  ranked  higher  than $D_j$.   The  two outputs $s_i$ and $s_j$ are mapped to a learned probability that $D_i \\triangleright D_j$: \n",
    "\n",
    "\n",
    "$$        P_{ij} = \\frac{1}{1 + e^{-\\sigma(s_i - s_j)}} $$\n",
    "  \n",
    "where $\\sigma$ is a parameter that determines the shape of the sigmoid. The loss of the RankNet model is the cross entropy cost function:\n",
    "\n",
    "$$        C = - \\bar{P}_{ij} \\log P_{ij} - (1-\\bar{P}_{ij}) \\log (1 - P_{ij}) $$\n",
    "\n",
    "As the name suggests, in the pairwise approach to LTR, we optimize a loss $l$ over pairs of documents. Let $S_{ij} \\in \\{0, \\pm1 \\}$ be equal to $1$ if the relevance of document $i$ is greater than document $j$; $-1$ if document $j$ is more relevant than document $i$; and 0 if they have the same relevance. This gives us $\\bar{P}_{ij} = \\frac{1}{2} (1 + S_{ij})$ so that $\\bar{P}_{ij} = 1$ if $D_i \\triangleright D_j$; $\\bar{P}_{ij} = 0$ if $D_j \\triangleright D_i$; and finally $\\bar{P}_{ij} = \\frac{1}{2}$ if the relevance is identical. This gives us:\n",
    "\n",
    "$$        C = \\frac{1}{2}(1- S_{ij})\\sigma(s_i - s_j) + \\log(1+ e^{-\\sigma(s_i - s_j)}) $$\n",
    "\n",
    "Now, consider a single query for which $n$ documents have been returned. Let the output scores of the ranker be $s_j$ ; $j=\\{1, \\dots, n \\}$, the model parameters be $w_k \\in \\mathbb{R}^W$, and let the set of pairs of document indices used for training be $\\mathcal{P}$. Then, the total cost is $C_T = \\sum_{i,j \\in \\mathcal{P}} C(s_i; s_j)$. \n",
    "\n",
    "\n",
    "\n",
    "- Implement RankNet. You should construct training samples by creating all possible pairs of documents for a given query and optimizing the loss above. Use the following parameters:\n",
    "  - Layers: $501 (input) \\rightarrow 256 \\rightarrow 1$, where each layer is a linear layer (`nn.Linear`) with a ReLu activation function (`nn.ReLu`) in between the layers. Use the default weight initialization scheme. (Hint: use `nn.Sequential` for a one-line forward function!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsY1vR_ldLIk"
   },
   "source": [
    "For the pairwise loss, we need to have a structured **dataloader** which detects the documents associated with a specific query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "txhRykOrdLIk"
   },
   "outputs": [],
   "source": [
    "class QueryGroupedLTRData(Dataset):\n",
    "    def __init__(self, data, split):\n",
    "        self.split = {\n",
    "            \"train\": data.train,\n",
    "            \"validation\": data.validation,\n",
    "            \"test\": data.test\n",
    "        }.get(split)\n",
    "        assert self.split is not None, \"Invalid split!\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.split.num_queries()\n",
    "\n",
    "    def __getitem__(self, q_i):\n",
    "        feature = torch.FloatTensor(self.split.query_feat(q_i))\n",
    "        labels = torch.FloatTensor(self.split.query_labels(q_i))\n",
    "        return feature, labels\n",
    "\n",
    "    \n",
    "## example\n",
    "train_data = QueryGroupedLTRData(data, \"train\")\n",
    "# this is how you would use it to quickly iterate over the train/val/test sets \n",
    "\n",
    "q_i = 300\n",
    "features_i, labels_i = train_data[q_i]\n",
    "print(f\"Query {q_i} has {len(features_i)} query-document pairs\")\n",
    "print(f\"Shape of features for Query {q_i}: {features_i.size()}\")\n",
    "print(f\"Shape of labels for Query {q_i}: {labels_i.size()}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sm62AA58dLIk"
   },
   "source": [
    "**Implementation (35 points):**\n",
    "First, implement the pairwaise loss, described above.\n",
    "\n",
    "**Rubric:**\n",
    " - Each ordering <i,j> combination is considered: 10 points\n",
    " - Proper application of the formula: 10 points\n",
    " - Mean loss: 5 points\n",
    " - Loss values for test cases as expected: 10 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9OJ2GSwpdLIk"
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (35 points)\n",
    "def pairwise_loss(scores, labels):\n",
    "    \"\"\"\n",
    "    Compute and return the pairwise loss *for a single query*. To compute this, compute the loss for each \n",
    "    ordering in a query, and then return the mean. Use sigma=1.\n",
    "    \n",
    "    For a query, consider all possible ways of comparing 2 document-query pairs.\n",
    "    \n",
    "    Hint: See the next cell for an example which should make it clear how the inputs look like\n",
    "    \n",
    "    scores: tensor of size [N, 1] (the output of a neural network), where N = length of <query, document> pairs\n",
    "    labels: tensor of size [N], contains the relevance labels \n",
    "    \"\"\"\n",
    "        \n",
    "    \n",
    "    \n",
    "    # if there's only one rating\n",
    "    if labels.size(0) < 2:\n",
    "        return None\n",
    "    \n",
    "    count = 0\n",
    "    # C_list = []\n",
    "    C_matrix = torch.zeros([len(scores), len(scores)])\n",
    "    S_ij = {}\n",
    "    for i_index, i_label  in enumerate(labels):\n",
    "        for j_index, j_label in enumerate(labels):\n",
    "            if i_label>j_label:\n",
    "                S_ij[(i_index,j_index)] = 1\n",
    "            if i_label<j_label:\n",
    "                S_ij[(i_index,j_index)] = -1\n",
    "            if i_label==j_label:\n",
    "                S_ij[(i_index,j_index)]= 0\n",
    "    for x in range(len(scores)):\n",
    "        for y in range(len(scores)):\n",
    "            if x != y:\n",
    "                C = ((0.5)*(1-S_ij[x,y])*(scores[x]-scores[y]))+np.log(1+np.exp(-1*(scores[x]-scores[y])))\n",
    "                # C_list.append(C)\n",
    "                C_matrix[x, y] = C\n",
    "                count+=1\n",
    "\n",
    "                \n",
    "    \n",
    "    # mean = sum(C_list)/len(C_list)\n",
    "    mean_matrix = C_matrix.sum()/count\n",
    "    \n",
    "    return mean_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fn6l4zsydLIk"
   },
   "outputs": [],
   "source": [
    "# Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-yOS3OHSdLIl"
   },
   "source": [
    "\\#### Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b95uuBGkdLIl"
   },
   "outputs": [],
   "source": [
    "scores_1 = torch.FloatTensor([0.2, 2.3, 4.5, 0.2, 1.0]).unsqueeze(1)\n",
    "labels_1 = torch.FloatTensor([1, 2, 3, 0, 4])\n",
    "\n",
    "\n",
    "scores_2 = torch.FloatTensor([3.2, 1.7]).unsqueeze(1)\n",
    "labels_2 = torch.FloatTensor([3, 1])\n",
    "\n",
    "assert torch.allclose(pairwise_loss(scores_1, labels_1), torch.tensor(0.6869), atol=1e-03)\n",
    "assert torch.allclose(pairwise_loss(scores_2, labels_2), torch.tensor(0.2014), atol=1e-03)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrXAfqe0dLIl"
   },
   "source": [
    "## Section 4: Pairwise: Speed-up RankNet (65 points) <a class=\"anchor\" id=\"SpairwiseLTR\"></a>\n",
    "\n",
    "[Back to TOC](#top)\n",
    "\n",
    "To speed up training of the previous model, we can consider a sped up version of the model, where instead of `.backward` on the loss, we use `torch.backward(lambda_i)`. \n",
    "\n",
    "The derivative of the total cost $C_T$ with respect to the model parameters $w_k$ is:\n",
    "\n",
    "$$        \\frac{\\partial C_T}{\\partial w_k} = \\sum_{(i,j) \\in \\mathcal{P}} \\frac{\\partial C(s_i, s_j)}{\\partial s_i} \\frac{\\partial s_i}{\\partial w_k} + \\frac{\\partial C(s_i, s_j)}{\\partial s_j} \\frac{\\partial s_j}{\\partial w_k} $$\n",
    "\n",
    "We can rewrite this sum by considering the set of indices $j$ , for which $\\{i,j\\}$ is a valid pair, denoted by $\\mathcal{P}_i$, and the set of document indices $\\mathcal{D}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C_T}{\\partial w_k} = \\sum_{i \\in \\mathcal{D}}\n",
    "\\frac{\\partial s_i}{\\partial w_k} \\sum_{j \\in \\mathcal{P}_i} \n",
    "\\frac{\\partial C(s_i, s_j)}{\\partial s_i} \n",
    "$$\n",
    "\n",
    "This sped of version of the algorithm first computes scores $s_i$ for all the documents. Then for each $j= 1, \\dots, n$, compute:\n",
    "\n",
    "$$\n",
    "\\lambda_{ij} = \\frac{\\partial C(s_i, s_j)}{\\partial s_i} = \\sigma \\bigg( \\frac{1}{2}(1 - S_{ij}) -  \\frac{1}{1 + e^{\\sigma(s_i -s_j))}} \\bigg) \\\\\n",
    "\\lambda_i = \\sum_{j \\in \\mathcal{P}_i} \\frac{\\partial C(s_i, s_j)}{\\partial s_i} = \\sum_{j \\in \\mathcal{P}_i} \\lambda_{ij}\n",
    "$$\n",
    "\n",
    "That gives us:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C_T}{\\partial w_k} = \\sum_{i \\in \\mathcal{D}}\n",
    "\\frac{\\partial s_i}{\\partial w_k} \\lambda_i\n",
    "$$\n",
    "\n",
    "This can be directly optimized in pytorch using: `torch.autograd.backward(scores, lambda_i)` \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ik-h2czidLIl"
   },
   "source": [
    "**Implementation (50 points):**\n",
    "Implement the sped-up version of pairwise loss, described above.\n",
    "\n",
    "**Rubric:**\n",
    " - Each ordering <i,j> combination is considered: 15 points\n",
    " - Proper application of the formula: 15 points\n",
    " - Loss values for test cases as expected: 20 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Agh3GPfjdLIl"
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (50 points)\n",
    "def compute_lambda_i(scores, labels):\n",
    "    \"\"\"\n",
    "    Compute \\lambda_i (defined in the previous cell). (assume sigma=1.)\n",
    "    \n",
    "    scores: tensor of size [N, 1] (the output of a neural network), where N = length of <query, document> pairs\n",
    "    labels: tensor of size [N], contains the relevance labels \n",
    "    \n",
    "    return: \\lambda_i, a tensor of shape: [N, 1]\n",
    "    \"\"\"\n",
    "        \n",
    "    # YOUR CODE HERE\n",
    "    N = len(labels)\n",
    "    labels = labels[None, :]\n",
    "    sij = labels.T - labels\n",
    "    sij[sij<-1] = -1\n",
    "    sij[sij>1] = 1\n",
    "    diff_scores = scores - scores.T\n",
    "\n",
    "    part1 = torch.exp(diff_scores)\n",
    "    part2 = part1.add(1)\n",
    "    part3 = 1 / part2\n",
    "\n",
    "    part4 = 1 - sij\n",
    "    part5 = 0.5 * part4\n",
    "\n",
    "    lambda_i_j = part5 - part3\n",
    "\n",
    "    lambda_i = torch.sum(lambda_i_j, 1)\n",
    "    lambda_i = torch.reshape(lambda_i, (N, 1))\n",
    "    \n",
    "    return lambda_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fMpBoaIqdLIl"
   },
   "outputs": [],
   "source": [
    "# Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDjqmSx5dLIl"
   },
   "source": [
    "\\#### Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-u7b_qVdLIm"
   },
   "outputs": [],
   "source": [
    "def mean_lambda(scores, labels):\n",
    "    return torch.stack([compute_lambda_i(scores, labels).mean(), torch.square(compute_lambda_i(scores, labels)).mean()])\n",
    "\n",
    "scores_1 = torch.FloatTensor([10.2, 0.3, 4.5, 2.0, -1.0]).unsqueeze(1)\n",
    "labels_1 = torch.FloatTensor([1, 2, 3, 0, 4])\n",
    "\n",
    "assert torch.allclose(mean_lambda(scores_1, labels_1), torch.tensor([0,5.5072]), atol=1e-03)\n",
    "\n",
    "scores_2 = torch.FloatTensor([3.2, 1.7]).unsqueeze(1)\n",
    "labels_2 = torch.FloatTensor([3, 1])\n",
    "\n",
    "assert torch.allclose(mean_lambda(scores_2, labels_2), torch.tensor([0,3.3279e-02]), atol=1e-03)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7w0ltT-dLIm"
   },
   "source": [
    "**Implementation (15 points):**\n",
    "Implement `train_batch_vector` function to compute the gradients (`torch.autograd.backward(scores, lambda_i)` function) and update the weights (`step()` function), using the specified loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xc3GO06udLIm"
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (15 points)\n",
    "\n",
    "def train_batch_vector(net, x, y, loss_fn, optimizer):\n",
    "    \"\"\"\n",
    "    Takes as input a batch of size N, i.e. feature matrix of size (N, 501), label vector of size (N), the loss function and optimizer for computing the gradients, and updates the weights of the model.\n",
    "    The loss function returns a vector of size [N, 1], the same as the output of network.\n",
    "\n",
    "    Input:  x: feature matrix, a [N, 501] tensor\n",
    "            y: label vector, a [N] tensor\n",
    "            loss_fn: an implementation of a loss function\n",
    "            optimizer: an optimizer for computing the gradients (we use Adam)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    optimizer.zero_grad()\n",
    "    scores = net(x)\n",
    "    loss = loss_fn(scores, y)\n",
    "    loss = torch.tensor(loss, requires_grad=True)\n",
    "    torch.autograd.backward(scores, loss)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8nPQ3wdVdLIm"
   },
   "outputs": [],
   "source": [
    "# Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAedFsi7dLIm"
   },
   "source": [
    "\\#### Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tjZHldpdLIm"
   },
   "source": [
    "##  Section 5: Listwise LTR (60 points) <a class=\"anchor\" id=\"listwiseLTR\"></a>\n",
    "\n",
    "[Back to TOC](#top)\n",
    "\n",
    "In this section, you will implement LambdaRank, a listwise approach to LTR. Consider the computation of $\\lambda$ for sped-up RankNet (that you've already implemented). $\\lambda$ here amounts to the 'force' on a document given its neighbours in the ranked list. The design of $\\lambda$ in LambdaRank is similar to RankNet, but is scaled by DCG gain from swapping the two documents in question. Let's suppose that the corresponding ranks of doucment $D_i$ and $D_j$ are $r_i$ and $r_j$ respectively. Given a ranking measure $IRM$, such as $NDCG$ or $ERR$, the lambda function in LambdaRank is defined as:\n",
    "\n",
    "\n",
    "$$        \\frac{\\partial C}{\\partial s_i} = \\sum_{j \\in D} \\lambda_{ij} \\cdot |\\bigtriangleup IRM (i,j)| $$\n",
    "\n",
    "Where $|\\bigtriangleup IRM(i,j)|$ is the absolute difference in $IRM$ after swapping the rank positions $r_i$ and $r_j$ while leaving everything else unchanged ($| \\cdot |$ denotes the absolute value). Note that we do not backpropogate $|\\bigtriangleup IRM|$, it is treated as a constant that scales the gradients. In this assignment we will use $|\\bigtriangleup NDCG|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRdTnKDhdLIn"
   },
   "source": [
    "**Implementation (60 points):**\n",
    "Implement the listwise loss.\n",
    "\n",
    "**Rubric:**\n",
    " - Each ordering <i,j> combination is considered: 15 points\n",
    " - Computing $|\\bigtriangleup NDCG|$: 15 points \n",
    " - Proper application of the formula: 15 points \n",
    " - Loss values as expected: 15 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZuAH2uSz9Me5"
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (60 points)\n",
    "def listwise_loss(scores, labels):\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute the LambdaRank loss. (assume sigma=1.)\n",
    "    \n",
    "    scores: tensor of size [N, 1] (the output of a neural network), where N = length of <query, document> pairs\n",
    "    labels: tensor of size [N], contains the relevance labels \n",
    "    \n",
    "    returns: a tensor of size [N, 1]\n",
    "    \"\"\"\n",
    "    N = len(labels)\n",
    "    labels = labels[None, :]\n",
    "    sij = labels.T - labels\n",
    "    sij[sij<-1] = -1\n",
    "    sij[sij>1] = 1\n",
    "    diff_scores = scores - scores.T\n",
    "\n",
    "    part1 = torch.exp(diff_scores)\n",
    "    part2 = part1.add(1)\n",
    "    part3 = 1 / part2\n",
    "\n",
    "    part4 = 1 - sij\n",
    "    part5 = 0.5 * part4\n",
    "\n",
    "    lambda_i_j = part5 - part3\n",
    "    ideal_labels = labels.sort(descending=True)[0][:].squeeze()\n",
    "\n",
    "    final = lambda_i_j * get_delta_ndcg(ideal_labels, labels, gpu=False)\n",
    "\n",
    "    final = torch.sum(final,1)\n",
    "    final = torch.reshape(final,(N,1))\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WY9T5isUBLmR"
   },
   "outputs": [],
   "source": [
    "def torch_ideal_dcg(batch_sorted_labels, gpu=False):\n",
    "\t'''\n",
    "\t:param sorted_labels: [batch, ranking_size]\n",
    "\t:return: [batch, 1]\n",
    "\t'''\n",
    "\tbatch_gains = torch.pow(2.0, batch_sorted_labels) - 1.0\n",
    "\tbatch_ranks = torch.arange(len(batch_sorted_labels))\n",
    "\n",
    "\tbatch_discounts =  torch.log2(2.0 + batch_ranks.type(torch.FloatTensor))\n",
    "\tbatch_ideal_dcg = torch.sum(batch_gains / batch_discounts)\n",
    "\n",
    "\treturn batch_ideal_dcg\n",
    "\n",
    "def get_delta_ndcg(batch_stds, batch_stds_sorted_via_preds, gpu):\n",
    "    '''\n",
    "    Delta-nDCG w.r.t. pairwise swapping of the currently predicted ltr_adhoc\n",
    "    :param batch_stds: the standard labels sorted in a descending order\n",
    "    :param batch_stds_sorted_via_preds: the standard labels sorted based on the corresponding predictions\n",
    "    :return:\n",
    "    '''\n",
    "    batch_idcgs = torch_ideal_dcg(batch_sorted_labels=batch_stds, gpu=gpu)                      # ideal discount cumulative gains\n",
    "    batch_gains = torch.pow(2.0, batch_stds_sorted_via_preds) - 1.0\n",
    "    batch_n_gains = batch_gains / batch_idcgs\n",
    "    batch_ng_diffs = torch.unsqueeze(batch_n_gains, dim=2) - torch.unsqueeze(batch_n_gains, dim=1)\n",
    "\n",
    "    batch_std_ranks = torch.arange(len(batch_stds))\n",
    "    batch_dists = 1.0 / torch.log2(batch_std_ranks + 2.0)   # discount co-efficients\n",
    "    batch_dists = torch.unsqueeze(batch_dists, dim=0)\n",
    "    batch_dists_diffs = torch.unsqueeze(batch_dists, dim=2) - torch.unsqueeze(batch_dists, dim=1)\n",
    "    batch_delta_ndcg = torch.abs(batch_ng_diffs) * torch.abs(batch_dists_diffs)  # absolute changes w.r.t. pairwise swapping\n",
    "    return batch_delta_ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EeqlXoKk046k"
   },
   "outputs": [],
   "source": [
    "def mean_lambda_list(scores, labels):\n",
    "    return torch.stack([listwise_loss(scores, labels).mean(), torch.square(listwise_loss(scores, labels)).mean()])\n",
    "\n",
    "\n",
    "scores_1 = torch.FloatTensor([10.2, 0.3, 4.5, 2.0, -1.0]).unsqueeze(1)\n",
    "labels_1 = torch.FloatTensor([1, 2, 3, 0, 4])\n",
    "# assert torch.allclose(mean_lambda_list(scores_1, labels_1), torch.tensor([0,0.1391]), atol=1e-03)\n",
    "\n",
    "scores_2 = torch.FloatTensor([3.2, 1.7]).unsqueeze(1)\n",
    "labels_2 = torch.FloatTensor([3, 1])\n",
    "assert torch.allclose(mean_lambda_list(scores_2, labels_2), torch.tensor([0,2.8024e-03]), atol=1e-03)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN56lo0CdLIn"
   },
   "source": [
    "\\#### Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-ofY0T7dLIn"
   },
   "outputs": [],
   "source": [
    "def mean_lambda_list(scores, labels):\n",
    "    print(torch.square(listwise_loss(scores, labels)).mean())\n",
    "    return torch.stack([listwise_loss(scores, labels).mean(), torch.square(listwise_loss(scores, labels)).mean()])\n",
    "\n",
    "scores_1 = torch.FloatTensor([10.2, 0.3, 4.5, 2.0, -1.0]).unsqueeze(1)\n",
    "labels_1 = torch.FloatTensor([1, 2, 3, 0, 4])\n",
    "# assert torch.allclose(mean_lambda_list(scores_1, labels_1), torch.tensor([0,0.1336]), atol=1e-03)\n",
    "\n",
    "scores_2 = torch.FloatTensor([3.2, 1.7]).unsqueeze(1)\n",
    "labels_2 = torch.FloatTensor([3, 1])\n",
    "assert torch.allclose(mean_lambda_list(scores_2, labels_2), torch.tensor([0,2.8024e-03]), atol=1e-03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNx3KzUBdLIn"
   },
   "source": [
    "## Section 6: Comparing Pointwise, Pairwise and Listwise (55 points) <a class=\"anchor\" id=\"evaluation1\"></a>\n",
    "\n",
    "[Back to TOC](#top)\n",
    "\n",
    "In the next few cells, we will compare the methods you've implemented. Helper functions are provided for you, which you can use to make some conclusions. You can modify the code as needed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXzEvhxmdLIn"
   },
   "source": [
    "First, let's have a function that plots the average scores of relevant (levels 3 and 4) and non-relevant (levels 0, 1, and 2) scores in terms of training epochs for different loss functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ooqf8djGdLIn"
   },
   "outputs": [],
   "source": [
    "loss_functions = {'pointwise':[pointwise_loss, train_batch],\n",
    "                 'pairwise':[compute_lambda_i, train_batch_vector],\n",
    "                 'listwise':[listwise_loss, train_batch_vector]}\n",
    "\n",
    "def plot_relevance_scores(batches, loss_function):\n",
    "    seed(420)\n",
    "    net = NeuralModule()\n",
    "    optimizer = Adam(net.parameters(), lr=0.005)\n",
    "    loss_fn = loss_functions[loss_function][0]\n",
    "    train_fn = loss_functions[loss_function][1]\n",
    "    \n",
    "    \n",
    "    train_batchs = batches[:len(batches)*3//4]\n",
    "    test_batchs = batches[len(batches)*3//4:]\n",
    "    \n",
    "    rel, nrel = [], []\n",
    "    \n",
    "    for i in range(100):\n",
    "        r, n = [], []\n",
    "        for x, y in test_batchs:\n",
    "            binary_rel = np.round(y/4,0)\n",
    "            scores = net(x)[:,0]\n",
    "            r.append(torch.sum(scores * binary_rel).detach().numpy() / torch.sum(binary_rel).detach().numpy())\n",
    "            n.append(torch.sum(scores * (1. - binary_rel)).detach().numpy() / torch.sum((1. - binary_rel)).detach().numpy())\n",
    "            \n",
    "        for x, y in train_batchs:\n",
    "            train_fn(net, x, y, loss_fn, optimizer)\n",
    "        rel.append(np.mean(np.array(r)))\n",
    "        nrel.append(np.mean(np.array(n)))\n",
    "        \n",
    "    \n",
    "        \n",
    "    plt.figure()\n",
    "    plt.suptitle(loss_function)\n",
    "    plt.plot(np.arange(10,len(rel)), rel[10:], label='relevant')\n",
    "    plt.plot(np.arange(10,len(nrel)), nrel[10:], label='non-relevant')\n",
    "    plt.legend()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTZbtCNWdLIo"
   },
   "source": [
    "For efficiency issues, we select a small number (83) of queries to test different loss functions.\n",
    "We split these queries into train and test with a 3:1 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IwypADHDdLIo"
   },
   "outputs": [],
   "source": [
    "batches = [train_data[i] for i in [181, 209, 233, 242, 259, 273, 327, 333, 377, 393, 410, 434, 452, 503, 529, 573, 581, 597, 625, 658, 683, 724, 756, 757, 801, 825, 826, 828, 874, 902, 1581, 1588, 1636, 1691, 1712, 1755, 1813, 1983, 2001, 2018, 2021, 2024, 2029, 2065, 2095, 2100, 2171, 2172, 2174, 2252, 2274, 2286, 2288, 2293, 2297, 2353, 2362, 2364, 2365, 2368, 2400, 2403, 2433, 2434, 2453, 2472, 2529, 2534, 2539, 2543, 2555, 2576, 2600, 2608, 2636, 2641, 2653, 2692, 2714, 2717, 2718, 2723, 2724]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gUywoUmdLIo"
   },
   "source": [
    "Next, we train a neural network with different loss functions on the selected queries.\n",
    "During training, we save the average scores of relevant and non-relevant validation items for each training epoch and plot them as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cl6x0q9680d3"
   },
   "outputs": [],
   "source": [
    "plot_relevance_scores(batches, 'listwise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmOBTXYxdLIo"
   },
   "outputs": [],
   "source": [
    "\n",
    "plot_relevance_scores(batches, 'pointwise')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bKseY2Fa8zPF"
   },
   "outputs": [],
   "source": [
    "plot_relevance_scores(batches, 'pairwise')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4l9eG1azdLIp"
   },
   "source": [
    "**Implementation (15 points):**\n",
    "Now implement a function similar to `plot_relevance_scores` that measures the NDCG@10 on the test split with different loss functions.\n",
    "Train your model for 10 epochs.\n",
    "For NDCG@10 use `evaluate.ndcg10(scores.detach().numpy(), y.detach().numpy())` for each query and average through all queries to obtain NDCG@10 for each loss function at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FI9sENJ0dLIp"
   },
   "outputs": [],
   "source": [
    "# TODO: Implement this! (15 points)\n",
    "\n",
    "\n",
    "def plot_ndcg10(batches, loss_function):\n",
    "    seed(420)\n",
    "    net = NeuralModule()\n",
    "    optimizer = Adam(net.parameters(), lr=0.005)\n",
    "    loss_fn = loss_functions[loss_function][0]\n",
    "    train_fn = loss_functions[loss_function][1]\n",
    "        \n",
    "    train_batchs = batches[:len(batches)*3//4]\n",
    "    test_batchs = batches[len(batches)*3//4:]\n",
    "    \n",
    "    ndcg = []\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    num_epochs = 10\n",
    "    print('train_batchs: ', train_batchs)\n",
    "    print('test_batchs: ', test_batchs)\n",
    "\n",
    "    for i in range(num_epochs):\n",
    "      batch_ndcg = []\n",
    "\n",
    "      for x, y in test_batchs:\n",
    "        print('test x: ', x)\n",
    "        print('test y: ', y)\n",
    "        scores = net(x)\n",
    "        batch_ndcg.append(evaluate.ndcg10(scores.detach().numpy(), y.detach().numpy()))\n",
    "        print('batch_ndcg: ', batch_ndcg)\n",
    "          \n",
    "      for x, y in train_batchs:\n",
    "        print('train x: ', x)\n",
    "        print('train y: ', y)\n",
    "        train_fn(net, x, y, loss_fn, optimizer)\n",
    "      \n",
    "      batch_mean_ndcg = np.mean(batch_ndcg)\n",
    "      ndcg.append(batch_mean_ndcg)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "#     plt.figure()\n",
    "    plt.plot(np.arange(len(ndcg)), ndcg, label=loss_function)\n",
    "    plt.legend()\n",
    "    \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tpK2smUddLIp"
   },
   "outputs": [],
   "source": [
    "# Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yy3pUgfdLIp"
   },
   "source": [
    "\\#### Please do not change this. This cell is used for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5rxn5wsrdLIp"
   },
   "outputs": [],
   "source": [
    "\n",
    "plot_ndcg10(batches, 'pointwise')\n",
    "\n",
    "plot_ndcg10(batches, 'pairwise')\n",
    "\n",
    "plot_ndcg10(batches, 'listwise')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBtoCagKdLIp"
   },
   "source": [
    "Write a conclusion in the next cell, considering (40 points):\n",
    "- rates of convergence\n",
    "- time complexity\n",
    "- distinguishing relevant and non-relevant items\n",
    "- performance for low data wrt NDCG@10\n",
    "- performance across queries\n",
    "- ... any other observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ggRS0kNNdLIp"
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jOjQt1j0dLIp"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of hw2_JTL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:IR]",
   "language": "python",
   "name": "conda-env-IR-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": "",
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "280px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
